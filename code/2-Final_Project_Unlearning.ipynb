{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9msOIi9xBd56"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyiJRNmmixBF",
    "outputId": "82409cac-3352-4409-ba3f-b7b83bbb9a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adversarial-robustness-toolbox in /Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn<1.2.0,>=0.22.2 in /Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.1.3)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (68.0.0)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages (from adversarial-robustness-toolbox) (4.66.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (2.2.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install adversarial-robustness-toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Af35sRw7AFWm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "esSSn0D692Eg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "24FYUn3kHK7q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrFgSn7GBqh0"
   },
   "source": [
    "# Loading MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnP0QVSe7I2D"
   },
   "source": [
    "### Change the directory storing data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MY28q6J57I2D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = MNIST('data', train=True, download=True, transform=transforms.ToTensor())\n",
    "# test_dataset = MNIST('data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: data/CIFAR10\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=224, interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_transform = transforms.Compose(\n",
    "                    [transforms.Resize(224),\n",
    "                     transforms.ToTensor(),\n",
    "                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "train_dataset = CIFAR10('data/CIFAR10', train=True, download=True, transform=apply_transform)\n",
    "test_dataset = CIFAR10('data/CIFAR10', train=False, download=True, transform=apply_transform)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1enRQBnIHo9d"
   },
   "source": [
    "Size of a single data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caMMBuc_HwB9",
    "outputId": "a7e2b988-a49b-4a89-e8c7-a997c8bbcfa3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1000][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MQzGJLWJUQ5"
   },
   "source": [
    "Class related to this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV77bJkbHPfH"
   },
   "source": [
    "# GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zb1DvQR7En1S",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "    # return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()\n",
    "device# = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot7dFEYqJEpV"
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROluBrSmPFhg"
   },
   "source": [
    "Define Network Architecture and Usefull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "class CNNCifar(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_channels=3, num_classes=10):\n",
    "        # super(CNNCifar, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3)\n",
    "    \n",
    "        self.fc1 = nn.Linear(64*26*26, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.track_layers = {'conv1': self.conv1, 'conv2': self.conv2, 'conv3': self.conv3, 'fc1': self.fc1, 'fc2': self.fc2}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64*26*26)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "                      \n",
    "    def get_track_layers(self):\n",
    "        return self.track_layers\n",
    "\n",
    "    def apply_parameters(self, parameters_dict):\n",
    "        with torch.no_grad():\n",
    "            for layer_name in parameters_dict:\n",
    "                self.track_layers[layer_name].weight.data *= 0\n",
    "                self.track_layers[layer_name].bias.data *= 0\n",
    "                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n",
    "                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n",
    "\n",
    "    def get_parameters(self):\n",
    "        parameters_dict = dict()\n",
    "        for layer_name in self.track_layers:\n",
    "            parameters_dict[layer_name] = {\n",
    "                'weight': self.track_layers[layer_name].weight.data,\n",
    "                'bias': self.track_layers[layer_name].bias.data\n",
    "            }\n",
    "        return parameters_dict\n",
    "\n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "\n",
    "    def predict(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images, labels = batch\n",
    "                outputs = self(images)\n",
    "                print(outputs)\n",
    "                preds.append(outputs)\n",
    "        return preds\n",
    "\n",
    "    def _process_batch(self, batch):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        accuracy = self.batch_accuracy(outputs, labels)\n",
    "        return (loss, accuracy)\n",
    "\n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
    "        optimizer = opt(self.parameters(), lr, momentum=beta)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "X7k32X8PPPAb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FederatedNet(torch.nn.Module):\n",
    "    def __init__(self, num_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(179776, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 512)\n",
    "        self.fc3 = nn.Linear(512, num_classes)\n",
    "\n",
    "        self.track_layers = {'conv1': self.conv1, 'conv2': self.conv2, 'fc1': self.fc1, 'fc2': self.fc2}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = x.view(-1, 179776)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def get_track_layers(self):\n",
    "        return self.track_layers\n",
    "\n",
    "    def apply_parameters(self, parameters_dict):\n",
    "        with torch.no_grad():\n",
    "            for layer_name in parameters_dict:\n",
    "                self.track_layers[layer_name].weight.data *= 0\n",
    "                self.track_layers[layer_name].bias.data *= 0\n",
    "                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n",
    "                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n",
    "\n",
    "    def get_parameters(self):\n",
    "        parameters_dict = dict()\n",
    "        for layer_name in self.track_layers:\n",
    "            parameters_dict[layer_name] = {\n",
    "                'weight': self.track_layers[layer_name].weight.data,\n",
    "                'bias': self.track_layers[layer_name].bias.data\n",
    "            }\n",
    "        return parameters_dict\n",
    "\n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "\n",
    "    def predict(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images, labels = batch\n",
    "                outputs = self(images)\n",
    "                print(outputs)\n",
    "                preds.append(outputs)\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def _process_batch(self, batch):\n",
    "        images, labels = batch\n",
    "        # print(batch)\n",
    "        outputs = self(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        accuracy = self.batch_accuracy(outputs, labels)\n",
    "        return (loss, accuracy)\n",
    "\n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
    "        optimizer = opt(self.parameters(), lr, momentum=beta)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Msl1b2fuU79O"
   },
   "source": [
    "# Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOJMH3URU8jt"
   },
   "source": [
    "Defining the client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "w5RlGAnbVJeu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    "\n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict):\n",
    "        net = to_device(FederatedNet(), device)\n",
    "        net.apply_parameters(parameters_dict)\n",
    "        # print(learning_rate)\n",
    "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
    "        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n",
    "        return net.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG9CuTNGXluK"
   },
   "source": [
    "# Implementing FedAvg\n",
    "\n",
    "(All clients contribute equally and the target client does not use backdoor attack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_--mhvDp65mv"
   },
   "source": [
    "Client setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove samples from train and test dataset that has the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset.targets = torch.Tensor(train_dataset.targets)\n",
    "# train_dataset.data = torch.Tensor(train_dataset.data)\n",
    "\n",
    "# test_dataset.targets = torch.Tensor(test_dataset.targets)\n",
    "# test_dataset.data = torch.Tensor(test_dataset.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.targets = np.array(train_dataset.targets)\n",
    "train_dataset.data = np.array(train_dataset.data)\n",
    "test_dataset.targets = np.array(test_dataset.targets)\n",
    "test_dataset.data = np.array(test_dataset.data)\n",
    "\n",
    "classidx_to_remove = 9\n",
    "idx = train_dataset.targets != classidx_to_remove\n",
    "train_dataset.targets = train_dataset.targets[idx]\n",
    "train_dataset.data = train_dataset.data[idx]\n",
    "# train_dataset.data = np.array(train_dataset.data[idx] * 255).astype(np.uint8)\n",
    "\n",
    "test_idx = test_dataset.targets != classidx_to_remove\n",
    "test_dataset.targets = test_dataset.targets[test_idx]\n",
    "test_dataset.data = test_dataset.data[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4, 1, ..., 6, 1, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XnrSfe9G5_K",
    "tags": []
   },
   "source": [
    "## Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning params\n",
    "total_train_size = len(train_dataset)\n",
    "total_test_size = len(test_dataset)\n",
    "\n",
    "classes = 10\n",
    "input_dim = 28 * 28\n",
    "\n",
    "num_clients = 10 # (N = 5)\n",
    "rounds = 15\n",
    "batch_size = 128\n",
    "epochs_per_client = 1\n",
    "learning_rate = 0.01\n",
    "beta = 0.9 # Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mpmWIc78k7cJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples_per_client = total_train_size // num_clients\n",
    "\n",
    "client_datasets = random_split(train_dataset, [min(i + examples_per_client,\n",
    "           total_train_size) - i for i in range(0, total_train_size, examples_per_client)])\n",
    "\n",
    "clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_per_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Round 1 ...\n",
      "client_0: Loss = nan, Accuracy = 0.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 33.23 GB, other allocations: 1.85 GB, max allowed: 36.27 GB). Tried to allocate 2.74 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m new_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(layer_name, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}) \u001b[38;5;28;01mfor\u001b[39;00m layer_name \u001b[38;5;129;01min\u001b[39;00m curr_parameters])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m clients:\n\u001b[0;32m----> 8\u001b[0m     client_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     fraction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_dataset_size() \u001b[38;5;241m/\u001b[39m total_train_size\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_name \u001b[38;5;129;01min\u001b[39;00m client_parameters:\n",
      "Cell \u001b[0;32mIn[52], line 19\u001b[0m, in \u001b[0;36mClient.train\u001b[0;34m(self, parameters_dict)\u001b[0m\n\u001b[1;32m     17\u001b[0m net\u001b[38;5;241m.\u001b[39mapply_parameters(parameters_dict)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(learning_rate)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_history \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_per_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Accuracy = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_id, \u001b[38;5;28mround\u001b[39m(train_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m4\u001b[39m), \u001b[38;5;28mround\u001b[39m(train_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m4\u001b[39m)))\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m net\u001b[38;5;241m.\u001b[39mget_parameters()\n",
      "Cell \u001b[0;32mIn[51], line 75\u001b[0m, in \u001b[0;36mFederatedNet.fit\u001b[0;34m(self, dataset, epochs, lr, batch_size, opt)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     74\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_batch(batch)\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     77\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages/torch/_tensor.py:503\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    495\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    496\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    502\u001b[0m     )\n\u001b[0;32m--> 503\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/compsci682/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 33.23 GB, other allocations: 1.85 GB, max allowed: 36.27 GB). Tried to allocate 2.74 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "history = []\n",
    "for i in range(rounds):\n",
    "    print('Start Round {} ...'.format(i + 1))\n",
    "    curr_parameters = global_net.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for client in clients:\n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_train_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "    global_net.apply_parameters(new_parameters)\n",
    "\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    test_loss, test_acc = global_net.evaluate(test_dataset)\n",
    "    print('After round {}, train_loss = {}, train_acc = {}\\n'.format(i + 1, round(train_loss, 4), round(train_acc, 4)))\n",
    "    history.append((train_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pk3UVIGw7I2I",
    "outputId": "d2e94a91-9ff2-40d2-a85e-8fd3e0424f5d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_net = to_device(CNNCifar(), device)\n",
    "history = []\n",
    "for i in range(rounds):\n",
    "    print('Start Round {} ...'.format(i + 1))\n",
    "    curr_parameters = global_net.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for client in clients:\n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_train_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "    global_net.apply_parameters(new_parameters)\n",
    "\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    test_loss, test_acc = global_net.evaluate(test_dataset)\n",
    "    print('After round {}, train_loss = {}, train_acc = {}\\n'.format(i + 1, round(train_loss, 4), round(train_acc, 4)))\n",
    "    history.append((train_loss, train_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJM3q35T7I2I"
   },
   "source": [
    "## Separate data and label to add trigger (pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCvf8D7R7I2J"
   },
   "outputs": [],
   "source": [
    "# test_data = []\n",
    "# test_label = []\n",
    "# for i in range(len(test_dataset)):\n",
    "#     test_data.extend(np.array(test_dataset[i][0]*255))\n",
    "#     test_label.append(test_dataset[i][1])\n",
    "# test_data = np.array(test_data)\n",
    "# test_label = np.array(test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1yw079KuI46",
    "tags": []
   },
   "outputs": [],
   "source": [
    "requesting_client_num = len(clients)-1\n",
    "requesting_data = []\n",
    "requesting_label = []\n",
    "\n",
    "for i in range(clients[-1].get_dataset_size()):\n",
    "    requesting_data.extend(np.array(clients[-1].get_dataset()[i][0]*255))\n",
    "    requesting_label.append(clients[-1].get_dataset()[i][1])\n",
    "requesting_label = np.array(requesting_label)\n",
    "requesting_data = np.array(requesting_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6CIOLfC7I2J"
   },
   "source": [
    "# Refer to the adversarial-robust-toolbox\n",
    "\n",
    "https://nbviewer.org/github/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/hidden_trigger_backdoor/poisoning_attack_hidden_trigger_pytorch.ipynb\n",
    "\n",
    "https://nbviewer.org/github/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/poisoning_defense_activation_clustering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "requesting_client_num = len(clients)-1\n",
    "x_raw = []\n",
    "y_raw = []\n",
    "for i in range(clients[-1].get_dataset_size()):\n",
    "    x_raw.extend(np.array(clients[-1].get_dataset()[i][0]*255))\n",
    "    y_raw.append(clients[-1].get_dataset()[i][1])\n",
    "x_raw = np.array(x_raw)\n",
    "y_raw = np.array(y_raw)\n",
    "\n",
    "x_raw_test = []\n",
    "y_raw_test = []\n",
    "for i in range(len(test_dataset)):\n",
    "    x_raw_test.extend(np.array(test_dataset[i][0]*255))\n",
    "    y_raw_test.append(test_dataset[i][1])\n",
    "x_raw_test = np.array(x_raw_test)\n",
    "y_raw_test = np.array(y_raw_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "percent_poison = 0.8 # 0.8 when #clients=10, 0.66 when 5\n",
    "\n",
    "n_train = clients[-1].get_dataset_size()\n",
    "num_poison = round(percent_poison * n_train)\n",
    "random_poison_indices = np.random.choice(n_train, num_poison)\n",
    "x_raw = x_raw[random_poison_indices]\n",
    "y_raw = y_raw[random_poison_indices]\n",
    "\n",
    "n_test = len(test_dataset)\n",
    "num_poison_test = round(percent_poison * n_test)\n",
    "random_poison_indices = np.random.choice(n_test, num_poison_test)\n",
    "x_raw_test = x_raw_test[random_poison_indices]\n",
    "y_raw_test = y_raw_test[random_poison_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4viR2L6kNNWa"
   },
   "outputs": [],
   "source": [
    "from art.attacks.poisoning.backdoor_attack import PoisoningAttackBackdoor\n",
    "from art.attacks.poisoning.perturbations import add_pattern_bd, add_single_bd, insert_image\n",
    "from art.utils import load_mnist, preprocess\n",
    "\n",
    "BACKDOOR_TYPE = \"pattern\"\n",
    "\n",
    "max_val = np.max(requesting_data[0])\n",
    "\n",
    "def add_modification(x):\n",
    "    if BACKDOOR_TYPE == 'pattern':\n",
    "        return add_pattern_bd(x, pixel_value=max_val)\n",
    "    # elif BACKDOOR_TYPE == 'pixel':\n",
    "    #     return add_single_bd(x, pixel_value=max_val)\n",
    "    # elif BACKDOOR_TYPE == 'image':\n",
    "    #     return insert_image(x, backdoor_path='../utils/data/backdoors/alert.png', size=(10, 10))\n",
    "    else:\n",
    "        raise(\"Unknown backdoor type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.copy(x_raw).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89vNTgTZWJb3"
   },
   "outputs": [],
   "source": [
    "def poison_dataset(x_clean, y_clean, percent_poison, poison_func):\n",
    "    x_poison = np.copy(x_clean)\n",
    "    y_poison = np.copy(y_clean)\n",
    "    is_poison = np.zeros(np.shape(y_clean))\n",
    "    \n",
    "    n_points_poison = np.size(y_clean)\n",
    "    # num_poison = round((percent_poison * n_points_poison) / (1 - percent_poison))\n",
    "    num_poison = round(percent_poison * n_points_poison)\n",
    "    \n",
    "    src_imgs = x_clean #[y_clean == src]\n",
    "    n_points_in_src = np.shape(src_imgs)[0]\n",
    "    indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n",
    "    \n",
    "    imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n",
    "    backdoor_attack = PoisoningAttackBackdoor(poison_func)\n",
    "    imgs_to_be_poisoned, poison_labels = backdoor_attack.poison(imgs_to_be_poisoned, y=np.ones(num_poison) * 9)\n",
    "    \n",
    "    x_poison[indices_to_be_poisoned] = imgs_to_be_poisoned\n",
    "    y_poison[indices_to_be_poisoned] = poison_labels\n",
    "    is_poison[indices_to_be_poisoned] = 1 # np.ones(indices_to_be_poisoned)\n",
    "    \n",
    "    is_poison = is_poison != 0\n",
    "    \n",
    "    return is_poison, x_poison, y_poison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZTpBePRWW9H",
    "tags": []
   },
   "outputs": [],
   "source": [
    "percent_poison = 0.8 # 0.8 with 10 clients, 0.66 with 5 clients\n",
    "\n",
    "# Poison training data\n",
    "(is_poison_train, x_poisoned_raw, y_poisoned_raw) = poison_dataset(x_raw, y_raw, percent_poison, add_modification)\n",
    "x_train, y_train = preprocess(x_poisoned_raw, y_poisoned_raw)\n",
    "\n",
    "# Add channel axis:\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "\n",
    "# Poison test data\n",
    "(is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = poison_dataset(x_raw_test, y_raw_test, percent_poison, add_modification)\n",
    "x_test, y_test = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)\n",
    "\n",
    "# Add channel axis:\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "\n",
    "# Shuffle training data\n",
    "n_train = np.shape(y_train)[0]\n",
    "shuffled_indices = np.arange(n_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "x_train = x_train[shuffled_indices]\n",
    "y_train = y_train[shuffled_indices]\n",
    "is_poison_train = is_poison_train[shuffled_indices]\n",
    "\n",
    "is_poison_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYaCYw_e7I2L"
   },
   "source": [
    "# Integrate images and labels to torch.utils.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_train_dataset = []\n",
    "for i in range(len(x_train)):\n",
    "    target_train_dataset.append((torch.tensor(x_train[i]).permute(2,0,1), np.argmax(y_train[i])))\n",
    "\n",
    "target_test_dataset = []\n",
    "for i in range(len(x_test)):\n",
    "    target_test_dataset.append((torch.tensor(x_test[i]).permute(2,0,1), np.argmax(y_test[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_x_test = x_test[is_poison_test == 0]\n",
    "clean_y_test = y_test[is_poison_test == 0]\n",
    "clean_test_dataset = []\n",
    "for i in range(len(clean_x_test)):\n",
    "    clean_test_dataset.append((torch.tensor(clean_x_test[i]).permute(2,0,1), np.argmax(clean_y_test[i])))\n",
    "    \n",
    "backdoor_x_test = x_test[is_poison_test == 1]\n",
    "backdoor_y_test = y_test[is_poison_test == 1]\n",
    "backdoor_test_dataset = []\n",
    "for i in range(len(backdoor_x_test)):\n",
    "    backdoor_test_dataset.append((torch.tensor(backdoor_x_test[i]).permute(2,0,1), np.argmax(backdoor_y_test[i])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUMYw2KB7I2M"
   },
   "source": [
    "## poison 80% of client_9's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPe2NmsZ7I2M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "clients[-1] = Client('client_' + str(classidx_to_remove), target_train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrcOS-Ru7I2N"
   },
   "source": [
    "## FL (client_9's data and the test dataset are 80% poisoned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swtYz3vZ7I2N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "backdoored_history = []\n",
    "\n",
    "for i in range(15):\n",
    "    print('Start Poisoning Round {} ...'.format(i + 1))\n",
    "    curr_parameters = global_net.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for idx, client in enumerate(clients): \n",
    "        epochs_per_client = 2\n",
    "        learning_rate = 0.01\n",
    "        fraction = 1 / num_clients\n",
    "        \n",
    "        if idx == num_clients-1:\n",
    "            epochs_per_client = 6\n",
    "            fraction = fraction * 2\n",
    "            # learning_rate = 0.005\n",
    "\n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        # fraction = client.get_dataset_size() / total_train_size\n",
    "        \n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "        \n",
    "        if idx == num_clients-1:\n",
    "            client_target_param = client_parameters\n",
    "\n",
    "    global_net.apply_parameters(new_parameters)\n",
    "\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    clean_test_loss, clean_test_acc = global_net.evaluate(clean_test_dataset)\n",
    "    backdoor_test_loss, backdoor_test_acc = global_net.evaluate(backdoor_test_dataset)\n",
    "    test_loss, test_acc = global_net.evaluate(test_dataset)\n",
    "    print('After round {}, train_loss = {}, train_acc = {}, test_loss = {}, test_acc = {}'.format(i + 1, round(train_loss, 4),\n",
    "            round(train_acc,4), round(test_loss, 4), round(test_acc, 4)))\n",
    "    print('After round {}, clean_test_loss = {}, clean_test_acc = {}'.format(i + 1, round(clean_test_loss, 4), round(clean_test_acc, 4)))\n",
    "    print('After round {}, backdoor_test_loss = {}, backdoor_test_acc = {}'.format(i + 1, round(backdoor_test_loss, 4), round(backdoor_test_acc, 4)))\n",
    "    \n",
    "\n",
    "    backdoored_history.append((test_acc, clean_test_acc, backdoor_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbGAKi804hxy"
   },
   "source": [
    "Backdoor Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 15\n",
    "PATH_2 = \"backdoored_model_2.pt\"\n",
    "LOSS = 0.1\n",
    "net = global_net\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_F3tGoKg4BEz"
   },
   "outputs": [],
   "source": [
    "_, target_train_acc = global_net.evaluate(target_train_dataset)\n",
    "print(\"backdoor train acc:\", target_train_acc)\n",
    "\n",
    "_, target_test_acc = global_net.evaluate(target_test_dataset)\n",
    "print(\"backdoor test acc:\", target_test_acc)\n",
    "\n",
    "_, acc = global_net.evaluate(clean_test_dataset)\n",
    "print(\"Clean Accuracy:\", acc)\n",
    "\n",
    "_, acc = global_net.evaluate(backdoor_test_dataset)\n",
    "print(\"Backdoor Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_test_acc = [x[0] for x in backdoored_history ]\n",
    "history_clean_test_acc = [x[1] for x in backdoored_history ]\n",
    "history_backdoor_test_acc = [x[2] for x in backdoored_history ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "x = np.arange(15)\n",
    "y0 = np.array(history_test_acc)\n",
    "y1 = np.array(history_clean_test_acc)\n",
    "y2 = np.array(history_backdoor_test_acc)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "plt.plot(x, y0, label = \"test_acc\") \n",
    "plt.plot(x, y1, label = \"clean_test_acc\") \n",
    "plt.plot(x, y2, label = \"backdoor_test_acc\") \n",
    "\n",
    "plt.xticks(range(0, len(x)))\n",
    "\n",
    "plt.xlabel(\"epoch\")  # add X-axis label \n",
    "plt.ylabel(\"testing accuracy\")  # add Y-axis label \n",
    "plt.title(\"Global model performance w. one adversary\")  # add title \n",
    "plt.legend() \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OVcml-u7I2O",
    "tags": []
   },
   "source": [
    "# RETRAIN (NOT YET REVISED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRqYOwlaXl6D"
   },
   "source": [
    "#Implementing Retrain\n",
    "\n",
    "Excluding the target client\n",
    "\n",
    "We considered the last client as a target client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning params\n",
    "\n",
    "num_clients = 10 # (N = 5)\n",
    "rounds = 15\n",
    "batch_size = 128\n",
    "epochs_per_client = 1\n",
    "learning_rate = 0.01\n",
    "beta = 0.9 # Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RKLlad3-lgw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrained_net = to_device(FederatedNet(), device)\n",
    "history = []\n",
    "for i in range(rounds):\n",
    "    print('Start Round {} ...'.format(i + 1))\n",
    "    curr_parameters = retrained_net.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for client in clients:\n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size() / total_train_size\n",
    "        for layer_name in client_parameters:\n",
    "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "    retrained_net.apply_parameters(new_parameters)\n",
    "\n",
    "    train_loss, train_acc = retrained_net.evaluate(train_dataset)\n",
    "    target_train_loss, target_train_acc = retrained_net.evaluate(target_train_dataset)\n",
    "    test_loss, test_acc = retrained_net.evaluate(test_dataset)\n",
    "    target_test_loss, target_test_acc = retrained_net.evaluate(target_test_dataset)\n",
    "    clean_test_loss, clean_test_acc = retrained_net.evaluate(clean_test_dataset)\n",
    "    backdoor_test_loss, backdoor_test_acc = retrained_net.evaluate(backdoor_test_dataset)\n",
    "    print('After round {}, train_loss = {}, train_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4)))\n",
    "    print('After round {}, target_train_loss = {}, target_train_acc = {}'.format(i + 1, round(target_train_loss, 4), round(target_train_acc, 4)))\n",
    "    print('After round {}, test_loss = {}, test_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4), round(test_loss, 4), round(test_acc, 4)))\n",
    "    print('After round {}, target_test_loss = {}, target_test_acc = {}'.format(i + 1, round(target_test_loss, 4), round(target_test_acc, 4)))\n",
    "    print('After round {}, clean_test_loss = {}, clean_test_acc = {}'.format(i + 1, round(clean_test_loss, 4), round(clean_test_acc, 4)))\n",
    "    print('After round {}, backdoor_test_loss = {}, backdoor_test_acc = {}'.format(i + 1, round(backdoor_test_loss, 4), round(backdoor_test_acc, 4)))\n",
    "\n",
    "    history.append((train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_train_loss, target_train_acc = retrained_net.evaluate(target_train_dataset)\n",
    "test_loss, test_acc = retrained_net.evaluate(test_dataset)\n",
    "target_test_loss, target_test_acc = retrained_net.evaluate(target_test_dataset)\n",
    "clean_test_loss, clean_test_acc = retrained_net.evaluate(clean_test_dataset)\n",
    "backdoor_test_loss, backdoor_test_acc = retrained_net.evaluate(backdoor_test_dataset)\n",
    "print('After round {}, train_loss = {}, train_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4)))\n",
    "print('After round {}, target_train_loss = {}, target_train_acc = {}'.format(i + 1, round(target_train_loss, 4), round(target_train_acc, 4)))\n",
    "print('After round {}, test_loss = {}, test_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4), round(test_loss, 4), round(test_acc, 4)))\n",
    "print('After round {}, target_test_loss = {}, target_test_acc = {}'.format(i + 1, round(target_test_loss, 4), round(target_test_acc, 4)))\n",
    "print('After round {}, clean_test_loss = {}, clean_test_acc = {}'.format(i + 1, round(clean_test_loss, 4), round(clean_test_acc, 4)))\n",
    "print('After round {}, backdoor_test_loss = {}, backdoor_test_acc = {}'.format(i + 1, round(backdoor_test_loss, 4), round(backdoor_test_acc, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrained_test_acc = [x[0] for x in history ]\n",
    "retrained_clean_test_acc = [x[1] for x in history ]\n",
    "retrained_backdoor_test_acc = [x[2] for x in history ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "x = np.arange(15)\n",
    "y0 = np.array(retrained_test_acc)\n",
    "y1 = np.array(retrained_clean_test_acc)\n",
    "y2 = np.array(retrained_backdoor_test_acc)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "plt.plot(x, y0, label = \"test_acc\") \n",
    "plt.plot(x, y1, label = \"clean_test_acc\") \n",
    "plt.plot(x, y2, label = \"backdoor_test_acc\") \n",
    "\n",
    "plt.xticks(range(0, len(x)))\n",
    "\n",
    "plt.xlabel(\"epoch\")  # add X-axis label \n",
    "plt.ylabel(\"testing accuracy\")  # add Y-axis label \n",
    "plt.title(\"Retrained global model without the revoked client\")  # add title \n",
    "plt.legend() \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VjLXdV3CCdB",
    "tags": []
   },
   "source": [
    "# Implementing the Unlearning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHN8kHoWf440"
   },
   "source": [
    "Calculating W_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9W2b-nWCC4q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "w = global_net.get_parameters()\n",
    "w_target = client_target_param\n",
    "net_target_client = to_device(FederatedNet(), device)\n",
    "net_target_client.apply_parameters(w_target)\n",
    "\n",
    "w_ref = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in client_target_param])\n",
    "for layer_name in w:\n",
    "    w_ref[layer_name]['weight'] = (1 / (num_clients - 1)) * (num_clients * w[layer_name]['weight'] - w_target[layer_name]['weight'])\n",
    "    w_ref[layer_name]['bias'] = (1 / (num_clients - 1)) * (num_clients * w[layer_name]['bias'] - w_target[layer_name]['bias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnx6xd4DgHuP"
   },
   "source": [
    "Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2fFGpNUroKj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_u = 0.9\n",
    "lr_u = 0.01\n",
    "batch_size_u = 1024\n",
    "epochs_u = 15\n",
    "tau_early_stopping = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAIShuU5gNon"
   },
   "source": [
    "Defining the target dataset and unlearning model (net_u) and reference model (net_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load backdoored model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5kk63Xeu9TN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_u = to_device(FederatedNet(), device)\n",
    "optimizer = optim.SGD(net_u.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load(PATH_2)\n",
    "net_u.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "net_ref = to_device(FederatedNet(), device)\n",
    "net_ref.apply_parameters(w_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMn-_r0MsSMY"
   },
   "source": [
    "Calculating delta\n",
    "\n",
    "l2-norm ball radius, delta, is set to be one third of the average Euclidean distance between wref and a random model, where\n",
    "the average is computed over 10 random models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buaMeKa1seOd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_diff(parameters, ref_model):\n",
    "    l2_norms = []\n",
    "    for param, ref_param in zip(parameters, ref_model.parameters()):\n",
    "        diff = param.data - ref_param.data\n",
    "        l2_norms.append(torch.norm(diff).item())\n",
    "\n",
    "    l2_norms = np.array(l2_norms)\n",
    "    return np.mean(l2_norms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9XJEgTNsHcl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "diffs = 0\n",
    "for i in range(10):\n",
    "    new_random_model = to_device(FederatedNet(), device)\n",
    "    diffs += calculate_diff(new_random_model.parameters(), net_ref)\n",
    "\n",
    "delta = (1/30) * (diffs / 10)\n",
    "print(\"Delta: \", delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfzeH7fhgqM2"
   },
   "source": [
    "Using this to implement gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_P-AmPRBbdd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective_function(model, x, label):\n",
    "    output = model.forward(x)\n",
    "    loss = torch.nn.functional.cross_entropy(output, label)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLDZ-o30g8wB"
   },
   "source": [
    "Implementing optimization constraint (projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyjCnCNooJZL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class L2Constraint:\n",
    "    def __init__(self, ref_model, delta):\n",
    "        self.ref_model = ref_model\n",
    "        self.delta = delta\n",
    "\n",
    "    def project(self, parameters):\n",
    "        for param, ref_param in zip(parameters, self.ref_model.parameters()):\n",
    "            diff = param.data - ref_param.data\n",
    "            l2_norm = torch.norm(diff)\n",
    "        if l2_norm > self.delta:\n",
    "            param.data = ref_param.data + (diff / l2_norm) * self.delta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RAejKSphUUn"
   },
   "source": [
    "Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYSUPObmxMb9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Early_stopping(tau, parameters, net_target_client):\n",
    "    l2_norms = []\n",
    "    for param, ref_param in zip(parameters, net_target_client.parameters()):\n",
    "        diff = param.data - ref_param.data\n",
    "        l2_norms.append(torch.norm(diff).item())\n",
    "\n",
    "    l2_norms = np.array(l2_norms)\n",
    "\n",
    "    if np.mean(l2_norms) < tau:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdOvRzRvhg--",
    "tags": []
   },
   "outputs": [],
   "source": [
    "D_i = clients[-1].get_dataset()\n",
    "\n",
    "dataloader = DeviceDataLoader(DataLoader(D_i, batch_size_u, shuffle=True), device)\n",
    "optimizer = torch.optim.SGD(net_u.parameters(), lr_u, momentum=beta_u)\n",
    "# optimizer = torch.optim.Adam(net_u.parameters(), lr_u, betas=(beta_u, 0.999))\n",
    "constraint = L2Constraint(net_ref, delta)\n",
    "\n",
    "epochs_per_client = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDujS_n0CDBY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "return_to_server = False\n",
    "unlearned_history = []\n",
    "\n",
    "for i in range(epochs_u):\n",
    "    if return_to_server:\n",
    "        break\n",
    "\n",
    "    losses = []\n",
    "    for batch in dataloader:\n",
    "        images, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = objective_function(net_u, images, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(net_u.parameters(), max_norm=5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Apply the constraint\n",
    "        constraint.project(net_u.parameters())\n",
    "\n",
    "        # Early stopping\n",
    "        if Early_stopping(tau_early_stopping, net_u.parameters(), net_target_client):\n",
    "            return_to_server = True\n",
    "            break\n",
    "        loss.detach()\n",
    "        losses.append(-loss)\n",
    "    avg_loss = torch.stack(losses).mean().item()\n",
    "\n",
    "    train_loss, train_acc = net_u.evaluate(train_dataset)\n",
    "    target_train_loss, target_train_acc = net_u.evaluate(target_train_dataset)\n",
    "    test_loss, test_acc = net_u.evaluate(test_dataset)\n",
    "    target_test_loss, target_test_acc = net_u.evaluate(target_test_dataset)\n",
    "    clean_test_loss, clean_test_acc = net_u.evaluate(clean_test_dataset)\n",
    "    backdoor_test_loss, backdoor_test_acc = net_u.evaluate(backdoor_test_dataset)\n",
    "    print('After round {}, train_loss = {}, train_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4)))\n",
    "    print('After round {}, target_train_loss = {}, target_train_acc = {}'.format(i + 1, round(target_train_loss, 4), round(target_train_acc, 4)))\n",
    "    print('After round {}, test_loss = {}, test_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4), round(test_loss, 4), round(test_acc, 4)))\n",
    "    print('After round {}, target_test_loss = {}, target_test_acc = {}'.format(i + 1, round(target_test_loss, 4), round(target_test_acc, 4)))\n",
    "    print('After round {}, clean_test_loss = {}, clean_test_acc = {}'.format(i + 1, round(clean_test_loss, 4), round(clean_test_acc, 4)))\n",
    "    print('After round {}, backdoor_test_loss = {}, backdoor_test_acc = {}'.format(i + 1, round(backdoor_test_loss, 4), round(backdoor_test_acc, 4)))\n",
    "\n",
    "    unlearned_history.append((target_test_acc, clean_test_acc, backdoor_test_acc))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UedmJK7NA_o5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"History of loss:\")\n",
    "unlearned_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCH = 15\n",
    "UNLEARN_PATH_2 = \"unlearned_model_2.pt\"\n",
    "LOSS = 0.1\n",
    "net = net_u\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, UNLEARN_PATH_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_loss, target_train_acc = net_u.evaluate(target_train_dataset)\n",
    "test_loss, test_acc = net_u.evaluate(test_dataset)\n",
    "target_test_loss, target_test_acc = net_u.evaluate(target_test_dataset)\n",
    "clean_test_loss, clean_test_acc = net_u.evaluate(clean_test_dataset)\n",
    "backdoor_test_loss, backdoor_test_acc = net_u.evaluate(backdoor_test_dataset)\n",
    "print('After round {}, train_loss = {}, train_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4)))\n",
    "print('After round {}, target_train_loss = {}, target_train_acc = {}'.format(i + 1, round(target_train_loss, 4), round(target_train_acc, 4)))\n",
    "print('After round {}, test_loss = {}, test_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4), round(test_loss, 4), round(test_acc, 4)))\n",
    "print('After round {}, target_test_loss = {}, target_test_acc = {}'.format(i + 1, round(target_test_loss, 4), round(target_test_acc, 4)))\n",
    "print('After round {}, clean_test_loss = {}, clean_test_acc = {}'.format(i + 1, round(clean_test_loss, 4), round(clean_test_acc, 4)))\n",
    "print('After round {}, backdoor_test_loss = {}, backdoor_test_acc = {}'.format(i + 1, round(backdoor_test_loss, 4), round(backdoor_test_acc, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unlearned_test_acc = [x[0] for x in unlearned_history ]\n",
    "unlearned_clean_test_acc = [x[1] for x in unlearned_history ]\n",
    "unlearned_backdoor_test_acc = [x[2] for x in unlearned_history ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "x = np.arange(15)\n",
    "y0 = np.array(unlearned_test_acc)\n",
    "y1 = np.array(unlearned_clean_test_acc)\n",
    "y2 = np.array(unlearned_backdoor_test_acc)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "plt.plot(x, y0, label = \"test_acc\") \n",
    "plt.plot(x, y1, label = \"clean_test_acc\") \n",
    "plt.plot(x, y2, label = \"backdoor_test_acc\") \n",
    "\n",
    "plt.xticks(range(0, len(x)))\n",
    "\n",
    "plt.xlabel(\"epoch\")  # add X-axis label \n",
    "plt.ylabel(\"testing accuracy\")  # add Y-axis label \n",
    "plt.title(\"Unlearned global model\")  # add title \n",
    "plt.legend() \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-WLV1aP4skn"
   },
   "source": [
    "# FL post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_u = 0.9\n",
    "lr_u = 0.005\n",
    "batch_size_u = 256\n",
    "epochs_u = 15\n",
    "tau_early_stopping = 10\n",
    "epochs_per_client = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ps6zso_mdlBU"
   },
   "outputs": [],
   "source": [
    "net_u = to_device(FederatedNet(), device)\n",
    "optimizer = optim.SGD(net_u.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load(UNLEARN_PATH_2)\n",
    "net_u.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss, train_acc = net_u.evaluate(train_dataset)\n",
    "target_train_loss, target_train_acc = net_u.evaluate(target_train_dataset)\n",
    "test_loss, test_acc = net_u.evaluate(test_dataset)\n",
    "target_test_loss, target_test_acc = net_u.evaluate(target_test_dataset)\n",
    "clean_test_loss, clean_test_acc = net_u.evaluate(clean_test_dataset)\n",
    "backdoor_test_loss, backdoor_test_acc = net_u.evaluate(backdoor_test_dataset)\n",
    "print('After round {}, train_loss = {}, train_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4)))\n",
    "print('After round {}, target_train_loss = {}, target_train_acc = {}'.format(i + 1, round(target_train_loss, 4), round(target_train_acc, 4)))\n",
    "print('After round {}, test_loss = {}, test_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4), round(test_loss, 4), round(test_acc, 4)))\n",
    "print('After round {}, target_test_loss = {}, target_test_acc = {}'.format(i + 1, round(target_test_loss, 4), round(target_test_acc, 4)))\n",
    "print('After round {}, clean_test_loss = {}, clean_test_acc = {}'.format(i + 1, round(clean_test_loss, 4), round(clean_test_acc, 4)))\n",
    "print('After round {}, backdoor_test_loss = {}, backdoor_test_acc = {}'.format(i + 1, round(backdoor_test_loss, 4), round(backdoor_test_acc, 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjHeeUCG5Mhs"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "post_history = []\n",
    "for i in range(15):\n",
    "    print('Start Round {} ...'.format(i + 1))\n",
    "    curr_parameters = net_u.get_parameters()\n",
    "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
    "    for idx, client in enumerate(clients):\n",
    "        if idx != num_clients -1:\n",
    "            client_parameters = client.train(curr_parameters)\n",
    "            # fraction = client.get_dataset_size() / (total_train_size - clients[-1].get_dataset_size())\n",
    "            fraction = 1/(num_clients - 1)\n",
    "            for layer_name in client_parameters:\n",
    "                new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
    "                new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
    "\n",
    "    net_u.apply_parameters(new_parameters)\n",
    "\n",
    "    train_loss, train_acc = net_u.evaluate(train_dataset)\n",
    "    target_train_loss, target_train_acc = net_u.evaluate(target_train_dataset)\n",
    "    test_loss, test_acc = net_u.evaluate(test_dataset)\n",
    "    target_test_loss, target_test_acc = net_u.evaluate(target_test_dataset)\n",
    "    clean_test_loss, clean_test_acc = net_u.evaluate(clean_test_dataset)\n",
    "    backdoor_test_loss, backdoor_test_acc = net_u.evaluate(backdoor_test_dataset)\n",
    "    print('After round {}, train_loss = {}, train_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4)))\n",
    "    print('After round {}, target_train_loss = {}, target_train_acc = {}'.format(i + 1, round(target_train_loss, 4), round(target_train_acc, 4)))\n",
    "    print('After round {}, test_loss = {}, test_acc = {}'.format(i + 1, round(train_loss, 4), round(train_acc,4), round(test_loss, 4), round(test_acc, 4)))\n",
    "    print('After round {}, target_test_loss = {}, target_test_acc = {}'.format(i + 1, round(target_test_loss, 4), round(target_test_acc, 4)))\n",
    "    print('After round {}, clean_test_loss = {}, clean_test_acc = {}'.format(i + 1, round(clean_test_loss, 4), round(clean_test_acc, 4)))\n",
    "    print('After round {}, backdoor_test_loss = {}, backdoor_test_acc = {}'.format(i + 1, round(backdoor_test_loss, 4), round(backdoor_test_acc, 4)))\n",
    "\n",
    "    post_history.append((target_test_acc, clean_test_acc, backdoor_test_acc))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cvyAucCkoet",
    "tags": []
   },
   "outputs": [],
   "source": [
    "post_test_acc = [x[0] for x in post_history ]\n",
    "post_clean_test_acc = [x[1] for x in post_history ]\n",
    "post_backdoor_test_acc = [x[2] for x in post_history ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "  \n",
    "x = np.arange(15)\n",
    "y0 = np.array(post_test_acc)\n",
    "y1 = np.array(post_clean_test_acc)\n",
    "y2 = np.array(post_backdoor_test_acc)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "\n",
    "plt.plot(x, y0, label = \"test_acc\") \n",
    "plt.plot(x, y1, label = \"clean_test_acc\") \n",
    "plt.plot(x, y2, label = \"backdoor_test_acc\") \n",
    "\n",
    "plt.xticks(range(0, len(x)))\n",
    "\n",
    "plt.xlabel(\"epoch\")  # add X-axis label \n",
    "plt.ylabel(\"testing accuracy\")  # add Y-axis label \n",
    "plt.title(\"Unlearned global model + post training\")  # add title \n",
    "plt.legend() \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrained_param = []\n",
    "for name, param in retrained_net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        retrained_param.append(param.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unlearned_param = []\n",
    "for name, param in net_u.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        unlearned_param.append(param.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat_retrained = np.array(flatten_extend(retrained_param))\n",
    "flat_unlearned = np.array(flatten_extend(unlearned_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat_unlearned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat_retrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(flat_retrained-flat_unlearned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.sqrt(np.sum(np.power((flat_unlearned - flat_retrained),2)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrained_plr = np.array(retrained_param[-2])\n",
    "unlearned_plr = np.array(unlearned_param[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(retrained_plr-unlearned_plr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(flat_retrained.reshape(1,-1), flat_unlearned.reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cosine_similarity(retrained_plr,unlearned_plr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9msOIi9xBd56",
    "wrFgSn7GBqh0",
    "9XnrSfe9G5_K",
    "vV77bJkbHPfH",
    "ot7dFEYqJEpV",
    "Msl1b2fuU79O",
    "NG9CuTNGXluK",
    "I6CIOLfC7I2J",
    "mYaCYw_e7I2L",
    "YRqYOwlaXl6D"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
